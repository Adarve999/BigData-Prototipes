# Contador de palabras con Apache Spark y Hadoop

Este proyecto demuestra cómo implementar un contador de palabras utilizando las tecnologías de Big Data Apache Spark y Hadoop. El proyecto contiene dos prototipos independientes, uno para Spark y otro para Hadoop, que procesan un archivo de texto y generan un output con la frecuencia de cada palabra en el archivo.

Este proyecto ha sido realizado con un carácter de investigación, con el objetivo de comparar y analizar las diferencias entre Apache Spark y Hadoop en términos de implementación y rendimiento.

## Estructura del proyecto

Cada prototipo contiene una clase Java para implementar el contador de palabras y dos archivos de documentación: un manual de usuario y un manual de instalación.

## Apache Spark

El prototipo de Apache Spark utiliza la biblioteca Spark Core para realizar el conteo de palabras. Consulta el manual de usuario y el manual de instalación dentro del directorio `spark-wordcount` para obtener más información sobre cómo utilizar y configurar este prototipo.

## Hadoop

El prototipo de Hadoop utiliza el framework MapReduce para realizar el conteo de palabras. Consulta el manual de usuario y el manual de instalación dentro del directorio `hadoop-wordcount` para obtener más información sobre cómo utilizar y configurar este prototipo.

## Investigación

Este proyecto es parte de una investigación en la que se analizan las diferencias entre Apache Spark y Hadoop. A través de este proyecto, exploramos las diferencias en la implementación, el rendimiento, la escalabilidad y otras características clave de ambas tecnologías.

## Licencia

Este proyecto está licenciado bajo los términos de la licencia [MIT](LICENSE).
