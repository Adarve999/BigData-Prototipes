# Word Counter with Apache Spark and Hadoop

This project demonstrates how to implement a word counter using the Big Data technologies Apache Spark and Hadoop. The project contains two independent prototypes, one for Spark and another for Hadoop, which process a text file and produce an output with the frequency of each word in the file.

This project has been conducted with a research character, with the aim to compare and analyze the differences between Apache Spark and Hadoop in terms of implementation and performance.

## Project Structure

Each prototype contains a Java class to implement the word counter and two documentation files: a user manual and an installation manual.

## Apache Spark

The Apache Spark prototype uses the Spark Core library to perform word counting. Refer to the user manual and installation manual inside the `spark-wordcount` directory for more information on how to use and set up this prototype.

## Hadoop

The Hadoop prototype uses the MapReduce framework to perform word counting. Refer to the user manual and installation manual inside the `hadoop-wordcount` directory for more information on how to use and set up this prototype.

## Research

This project is part of research in which we analyze the differences between Apache Spark and Hadoop. Through this project, we explore differences in implementation, performance, scalability, and other key features of both technologies.

## License

This project is licensed under the terms of the [GNU General Public License v3.0](LICENSE).

# Contador de palabras con Apache Spark y Hadoop

Este proyecto demuestra cómo implementar un contador de palabras utilizando las tecnologías de Big Data Apache Spark y Hadoop. El proyecto contiene dos prototipos independientes, uno para Spark y otro para Hadoop, que procesan un archivo de texto y generan un output con la frecuencia de cada palabra en el archivo.

Este proyecto ha sido realizado con un carácter de investigación, con el objetivo de comparar y analizar las diferencias entre Apache Spark y Hadoop en términos de implementación y rendimiento.

## Estructura del proyecto

Cada prototipo contiene una clase Java para implementar el contador de palabras y dos archivos de documentación: un manual de usuario y un manual de instalación.

## Apache Spark

El prototipo de Apache Spark utiliza la biblioteca Spark Core para realizar el conteo de palabras. Consulta el manual de usuario y el manual de instalación dentro del directorio `spark-wordcount` para obtener más información sobre cómo utilizar y configurar este prototipo.

## Hadoop

El prototipo de Hadoop utiliza el framework MapReduce para realizar el conteo de palabras. Consulta el manual de usuario y el manual de instalación dentro del directorio `hadoop-wordcount` para obtener más información sobre cómo utilizar y configurar este prototipo.

## Investigación

Este proyecto es parte de una investigación en la que se analizan las diferencias entre Apache Spark y Hadoop. A través de este proyecto, exploramos las diferencias en la implementación, el rendimiento, la escalabilidad y otras características clave de ambas tecnologías.

## Licencia

Este proyecto está licenciado bajo los términos de la licencia [GNU General Public License v3.0](LICENSE).
